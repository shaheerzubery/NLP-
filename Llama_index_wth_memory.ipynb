{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaheerzubery/NLP-/blob/main/Llama_index_wth_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5y2ah_PhaJk"
      },
      "source": [
        "Our today's lecture is about Llama index with agent memory, thhis will work like a proper chatbo on your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEhGXOlwbEsL"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G5yWy6WkdoUO"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c_14CPaQbKgb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = openai_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf1tuHZVhsea"
      },
      "source": [
        "Import the  required dependencies, since llama index comes with langchain so we don't need to install them seperately, we are using pdf loader, you can use other loaders as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L2di2mah5k-"
      },
      "source": [
        "This is our loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MZ255QtVbM-v"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from llama_index import download_loader\n",
        "\n",
        "PDFReader = download_loader(\"PDFReader\")\n",
        "\n",
        "loader = PDFReader()\n",
        "documents = loader.load_data(file=Path('/content/final-final-cares-act.pdf'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulYjjDrXbO2s"
      },
      "outputs": [],
      "source": [
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can see our document is loaded, now we will make embedding : text embedding ada 002 and store it in vectore store"
      ],
      "metadata": {
        "id": "ysTMBvpCPnRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TMJVINiCbUH6"
      },
      "outputs": [],
      "source": [
        "from llama_index import GPTVectorStoreIndex\n",
        " \n",
        "index = GPTVectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This query engine check the similarity btw the prompt and data"
      ],
      "metadata": {
        "id": "C9k9sudJQNnZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0CzIAw7bWE6",
        "outputId": "fdd4eefe-17dd-41ff-e157-47138f2ad285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Delays in obtaining necessary components for production.\n",
            "2. Shortages of raw materials needed for production.\n",
            "3. Unavailability of specialized tools and equipment.\n",
            "4. Unavailability of skilled labor.\n",
            "5. Unavailability of specialized parts.\n",
            "6. Unavailability of specialized software.\n",
            "7. Unavailability of specialized testing equipment.\n",
            "8. Unavailability of specialized safety equipment.\n",
            "9. Unavailability of specialized production machinery.\n",
            "10. Unavailability of specialized production processes.\n",
            "11. Unavailability of specialized production techniques.\n",
            "12. Unavailability of specialized production materials.\n",
            "13. Unavailability of specialized production components.\n",
            "14. Unavailability of specialized production tools.\n",
            "15. Unavailability of specialized production equipment.\n",
            "16. Unavailability of specialized production supplies.\n",
            "17. Unavailability of specialized production parts.\n",
            "18. Unavailability of specialized production software.\n",
            "19. Unavailability of specialized production testing equipment.\n",
            "20. Unavailability of specialized production safety equipment.\n",
            "21. Unavailability of specialized production machinery.\n",
            "22. Unavailability of specialized production processes.\n",
            "23. Unavailability of specialized production techniques.\n",
            "24. Unavailability of specialized production materials.\n",
            "25. Unavailability of specialized production components.\n",
            "26\n"
          ]
        }
      ],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Give me 35 examples of Electrical Equipment, Appliance, and Component Manufacturing disruption\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Memory"
      ],
      "metadata": {
        "id": "qEaww07_QWup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code helps to add memory on our model"
      ],
      "metadata": {
        "id": "NOg6Pgk5QZZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "keTzWxBlchBa"
      },
      "outputs": [],
      "source": [
        "from llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool, LlamaToolkit\n",
        "\n",
        "index_configs = []\n",
        "\n",
        "tool_config = IndexToolConfig(\n",
        "    query_engine=query_engine, \n",
        "    name=f\"Vector Index\",\n",
        "    description=f\"useful for when you want to answer queries about X\",\n",
        "    tool_kwargs={\"return_direct\": True}\n",
        ")\n",
        "index_configs.append(tool_config)\n",
        "tool = LlamaIndexTool.from_tool_config(tool_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ldX9SoWocm1q"
      },
      "outputs": [],
      "source": [
        "toolkit = LlamaToolkit(\n",
        "    index_configs=index_configs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCBz3eZEc81r"
      },
      "outputs": [],
      "source": [
        "from llama_index.langchain_helpers.agents import create_llama_chat_agent\n",
        "from langchain import OpenAI\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm=OpenAI(temperature=0)\n",
        "agent_chain = create_llama_chat_agent(\n",
        "    toolkit,\n",
        "    llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"User: \")\n",
        "    response = agent_chain.run(input=text_input)\n",
        "    print(f'Agent: {response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is how you can add memory to your Question and Answer model"
      ],
      "metadata": {
        "id": "5WjFKEPBQ8rI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UlIMarKbaWg"
      },
      "source": [
        "### Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5I_e3WNgbaEt"
      },
      "outputs": [],
      "source": [
        "openai_key = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gGAYTkuGbbh2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmXPgdxf0gnF9kpqi4CRwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}